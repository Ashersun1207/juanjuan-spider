#!/usr/bin/env python3
"""
juanjuan-spider CLI ğŸ•·ï¸

ç”¨æ³•:
  python3 scrape.py <URL> [é€‰é¡¹]

é€‰é¡¹:
  --proxy URL        ä»£ç†åœ°å€ï¼ˆé»˜è®¤ http://127.0.0.1:7897ï¼‰
  --no-proxy         ä¸ä½¿ç”¨ä»£ç†
  --wait SEC         é¡µé¢åŠ è½½åé¢å¤–ç­‰å¾…ç§’æ•°
  --selector CSS     åªæŠ“å–åŒ¹é…çš„ CSS é€‰æ‹©å™¨å†…å®¹
  --output FILE      è¾“å‡ºåˆ°æ–‡ä»¶ï¼ˆé»˜è®¤ stdoutï¼‰
  --format FMT       è¾“å‡ºæ ¼å¼: markdown / html / text / screenshot / fit
  --scroll           è‡ªåŠ¨æ»šåŠ¨åˆ°åº•éƒ¨
  --headed           æœ‰å¤´æ¨¡å¼ï¼ˆè°ƒè¯•ç”¨ï¼‰
  --cookie FILE      åŠ è½½ cookie JSON æ–‡ä»¶
  --js CODE          é¡µé¢åŠ è½½åæ‰§è¡Œçš„ JS ä»£ç 
  --max-chars N      è¾“å‡ºæœ€å¤§å­—ç¬¦æ•°
  --timeout SEC      é¡µé¢åŠ è½½è¶…æ—¶ç§’æ•°ï¼ˆé»˜è®¤ 30ï¼‰
  --stealth          å¯ç”¨åæ£€æµ‹ï¼ˆé»˜è®¤ï¼‰
  --no-stealth       å…³é—­åæ£€æµ‹
  --save             ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨ï¼ˆSQLite + markdown æ–‡ä»¶ï¼‰
  --no-cache         å¿½ç•¥ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æŠ“
  --verbose          æ˜¾ç¤ºè¯¦ç»†æ—¥å¿—
"""

import argparse
import asyncio
import os
import re
import sys
import warnings
from pathlib import Path

warnings.filterwarnings("ignore", category=UserWarning)
os.environ.setdefault("CRAWL4AI_LOG_LEVEL", "ERROR")


def parse_args():
    p = argparse.ArgumentParser(description="juanjuan-spider ğŸ•·ï¸ é€šç”¨ç½‘é¡µæŠ“å–")
    p.add_argument("url", help="ç›®æ ‡ URL")
    p.add_argument("--proxy", default="http://127.0.0.1:7897")
    p.add_argument("--no-proxy", action="store_true")
    p.add_argument("--wait", type=float, default=0)
    p.add_argument("--selector")
    p.add_argument("--output", "-o")
    p.add_argument("--format", "-f", default="markdown",
                   choices=["markdown", "html", "text", "screenshot", "fit"])
    p.add_argument("--scroll", action="store_true")
    p.add_argument("--headed", action="store_true")
    p.add_argument("--cookie")
    p.add_argument("--js")
    p.add_argument("--max-chars", type=int, default=0)
    p.add_argument("--timeout", type=int, default=30)
    p.add_argument("--stealth", action="store_true", default=True)
    p.add_argument("--no-stealth", dest="stealth", action="store_false")
    p.add_argument("--save", action="store_true", help="ä¿å­˜åˆ°æœ¬åœ°å­˜å‚¨")
    p.add_argument("--no-cache", action="store_true", help="å¿½ç•¥ç¼“å­˜å¼ºåˆ¶é‡æŠ“")
    p.add_argument("--verbose", action="store_true")
    return p.parse_args()


async def run(args):
    from spider.core.engine import FetchConfig
    from spider.main import crawl

    fc = FetchConfig(
        proxy=args.proxy if not args.no_proxy else None,
        timeout=args.timeout,
        stealth=args.stealth,
        headless=not args.headed,
        wait=args.wait,
        scroll=args.scroll,
        selector=args.selector,
        js_code=args.js,
        cookie_file=args.cookie,
        verbose=args.verbose,
    )

    result = await crawl(
        args.url,
        save=args.save,
        no_cache=args.no_cache,
        fetch_config=fc,
    )

    if result.status == "failed":
        print(f"âŒ æŠ“å–å¤±è´¥: {result.error}", file=sys.stderr)
        sys.exit(1)

    if result.status == "cached":
        print("ğŸ“¦ å‘½ä¸­ç¼“å­˜", file=sys.stderr)

    # æˆªå›¾
    if args.format == "screenshot":
        if result.screenshot:
            out_path = args.output or "screenshot.png"
            Path(out_path).write_bytes(result.screenshot)
            print(f"âœ… æˆªå›¾å·²ä¿å­˜: {out_path}", file=sys.stderr)
        else:
            print("âŒ æˆªå›¾å¤±è´¥", file=sys.stderr)
            sys.exit(1)
        return

    # é€‰æ‹©è¾“å‡ºå†…å®¹
    if args.format == "html":
        output = result.html
    elif args.format == "fit":
        output = result.fit_markdown or result.markdown
    elif args.format == "text":
        output = re.sub(r'!?\[([^\]]*)\]\([^)]+\)', r'\1', result.markdown)
        output = re.sub(r'[#*_`~]', '', output)
    else:
        output = result.markdown

    # æˆªæ–­
    if args.max_chars > 0 and len(output) > args.max_chars:
        output = output[:args.max_chars] + f"\n\n... (æˆªæ–­äº {args.max_chars} å­—ç¬¦)"

    # è¾“å‡º
    if args.output:
        Path(args.output).write_text(output, encoding="utf-8")
        print(f"âœ… å·²ä¿å­˜: {args.output} ({len(output)} å­—ç¬¦)", file=sys.stderr)
    else:
        print(output)

    # æ‰“å°å…ƒä¿¡æ¯
    if args.verbose:
        print(f"\n--- å…ƒä¿¡æ¯ ---", file=sys.stderr)
        print(f"å¼•æ“: {result.engine}", file=sys.stderr)
        print(f"çŠ¶æ€: {result.status}", file=sys.stderr)
        print(f"è€—æ—¶: {result.duration_ms}ms", file=sys.stderr)
        print(f"å­—ç¬¦æ•°: {result.char_count}", file=sys.stderr)
        print(f"åŸŸå: {result.domain}", file=sys.stderr)
        print(f"å†…å®¹å“ˆå¸Œ: {result.content_hash}", file=sys.stderr)


def main():
    args = parse_args()
    if not args.verbose:
        os.environ["CRAWL4AI_LOG_LEVEL"] = "ERROR"
    asyncio.run(run(args))


if __name__ == "__main__":
    main()
