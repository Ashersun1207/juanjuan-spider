# Crawlee Python æ¶æ„æ·±åº¦åˆ†æ

> åŸºäº crawlee-python æºç å®é™…é˜…è¯»ï¼Œä¸º juanjuan-spider é¡¹ç›®æä¾›æ¶æ„å‚è€ƒã€‚
> æ—¥æœŸ: 2025-02-25

---

## 1. åˆ†å±‚æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      ç”¨æˆ·å±‚ (User API)                           â”‚
â”‚  crawler.run() / @router.handler('label') / await ctx.push_data â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    å…·ä½“ Crawler å±‚                                â”‚
â”‚  PlaywrightCrawler / HttpCrawler / BeautifulSoupCrawler / ...    â”‚
â”‚  æ¯ç§ Crawler é€šè¿‡ ContextPipeline.compose() æ³¨å…¥è‡ªå·±çš„ä¸­é—´ä»¶é“¾    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              AbstractHttpCrawler å±‚ (HTTP é€šç”¨é€»è¾‘)              â”‚
â”‚  _create_static_content_crawler_pipeline():                      â”‚
â”‚    pre_nav_hooks â†’ make_http_request â†’ status_code_check         â”‚
â”‚    â†’ parse_response â†’ blocked_check                              â”‚
â”‚  å«: HttpParser æŠ½è±¡ / enqueue_links / extract_links             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                   BasicCrawler å±‚ (æ ¸å¿ƒå¼•æ“)                     â”‚
â”‚  è¯·æ±‚è°ƒåº¦ / é‡è¯• / Session ç®¡ç† / è‡ªåŠ¨ç¼©æ”¾ / ç»Ÿè®¡ / å­˜å‚¨         â”‚
â”‚  æ ¸å¿ƒå¾ªç¯: AutoscaledPool â†’ fetch_request â†’ context_pipeline     â”‚
â”‚           â†’ router(handler) â†’ commit_result                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    åŸºç¡€è®¾æ–½å±‚ (Infrastructure)                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ StorageClientâ”‚ â”‚ EventManager â”‚ â”‚ Configuration            â”‚ â”‚
â”‚  â”‚ (FS/Mem/SQL/ â”‚ â”‚ (Local/     â”‚ â”‚ (pydantic-settings       â”‚ â”‚
â”‚  â”‚  Redis)      â”‚ â”‚  Platform)  â”‚ â”‚  + env vars)             â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚ HttpClient   â”‚ â”‚ SessionPool  â”‚ â”‚ ProxyConfiguration       â”‚ â”‚
â”‚  â”‚ (Impit/Curl/ â”‚ â”‚ + Session    â”‚ â”‚ (è½®è¯¢/åˆ†å±‚/è‡ªå®šä¹‰)        â”‚ â”‚
â”‚  â”‚  Playwright) â”‚ â”‚              â”‚ â”‚                          â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚
â”‚  â”‚ ServiceLocator (å…¨å±€å•ä¾‹, æƒ°æ€§åˆå§‹åŒ–, ç®¡ç†ä¸Šè¿°æ‰€æœ‰æœåŠ¡)         â”‚â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç ä¾æ®**: `src/crawlee/_service_locator.py` ä¸­ `ServiceLocator` ç±»ç®¡ç†ä¸‰ä¸ªæ ¸å¿ƒæœåŠ¡:
- `Configuration` â€” å…¨å±€é…ç½®
- `EventManager` â€” äº‹ä»¶ç®¡ç†
- `StorageClient` â€” å­˜å‚¨åç«¯

---

## 2. æ¯å±‚çš„æ ¸å¿ƒç±»å’ŒèŒè´£

### 2.1 BasicCrawler â€” æ ¸å¿ƒå¼•æ“ (`_basic_crawler.py`, ~1679è¡Œ)

**èŒè´£**: è¯·æ±‚è°ƒåº¦ã€é‡è¯•ã€ç”Ÿå‘½å‘¨æœŸç®¡ç†

**æ ¸å¿ƒå±æ€§**:
```python
class BasicCrawler(Generic[TCrawlingContext, TStatisticsState]):
    _request_manager: RequestManager          # è¯·æ±‚é˜Ÿåˆ—
    _session_pool: SessionPool                # Session æ± 
    _proxy_configuration: ProxyConfiguration  # ä»£ç†é…ç½®
    _http_client: HttpClient                  # HTTP å®¢æˆ·ç«¯
    _router: Router[TCrawlingContext]          # è¯·æ±‚è·¯ç”±
    _context_pipeline: ContextPipeline        # ä¸­é—´ä»¶ç®¡é“
    _autoscaled_pool: AutoscaledPool          # è‡ªåŠ¨ç¼©æ”¾å¹¶å‘æ± 
    _statistics: Statistics                   # ç»Ÿè®¡
    _snapshotter: Snapshotter                 # ç³»ç»Ÿèµ„æºå¿«ç…§
```

**å…³é”®è®¾è®¡**: BasicCrawler æ˜¯ **æ³›å‹ç±»**ï¼Œ`TCrawlingContext` ç±»å‹å‚æ•°å†³å®šäº†ç”¨æˆ· handler æ”¶åˆ°ä»€ä¹ˆæ ·çš„ä¸Šä¸‹æ–‡ã€‚å­ç±»é€šè¿‡ `_context_pipeline` é€æ­¥å¢å¼ºä¸Šä¸‹æ–‡ã€‚

### 2.2 AbstractHttpCrawler â€” HTTP æŠ½è±¡å±‚ (`_abstract_http_crawler.py`)

**èŒè´£**: HTTP è¯·æ±‚çš„å‘é€ã€å“åº”è§£æã€é“¾æ¥æå–çš„é€šç”¨é€»è¾‘

**ä¸­é—´ä»¶ç®¡é“** (`_create_static_content_crawler_pipeline`):
```python
ContextPipeline()
    .compose(self._execute_pre_navigation_hooks)   # å‰ç½®é’©å­
    .compose(self._make_http_request)              # å‘ HTTP è¯·æ±‚
    .compose(self._handle_status_code_response)    # çŠ¶æ€ç æ£€æŸ¥
    .compose(self._parse_http_response)            # è§£æå“åº”
    .compose(self._handle_blocked_request_by_content)  # åçˆ¬æ£€æµ‹
```

**å…³é”®**: `AbstractHttpParser` æŠ½è±¡è§£æå™¨ â€” BeautifulSoup / Parsel / åŸå§‹HTTP åˆ†åˆ«æœ‰å„è‡ªçš„ Parser å®ç°ã€‚

### 2.3 PlaywrightCrawler â€” æµè§ˆå™¨å±‚ (`_playwright_crawler.py`)

**ä¸­é—´ä»¶ç®¡é“**:
```python
ContextPipeline()
    .compose(self._open_page)                        # æ‰“å¼€æµè§ˆå™¨é¡µé¢
    .compose(self._navigate)                         # å¯¼èˆªåˆ° URL
    .compose(self._handle_status_code_response)      # çŠ¶æ€ç æ£€æŸ¥
    .compose(self._handle_blocked_request_by_content) # åçˆ¬æ£€æµ‹
```

**å…³é”®**: `BrowserPool` ç®¡ç†æµè§ˆå™¨å®ä¾‹å’Œé¡µé¢ï¼Œæ”¯æŒæŒ‡çº¹ç”Ÿæˆ (`FingerprintGenerator`)ã€‚

### 2.4 ContextPipeline â€” ä¸­é—´ä»¶ç®¡é“ (`_context_pipeline.py`)

è¿™æ˜¯ Crawlee æœ€ç²¾å¦™çš„è®¾è®¡ä¹‹ä¸€ï¼š

```python
class ContextPipeline(Generic[TCrawlingContext]):
    def compose(self, middleware) -> ContextPipeline[TMiddlewareCrawlingContext]:
        """é“¾å¼æ³¨å†Œä¸­é—´ä»¶ï¼Œè¿”å›æ–°çš„ Pipeline å®ä¾‹ï¼ˆä¸å¯å˜é“¾è¡¨ï¼‰"""
        return ContextPipeline(
            _middleware=middleware,
            _parent=self,
        )

    async def __call__(self, crawling_context, final_context_consumer):
        """æŒ‰åºæ‰§è¡Œä¸­é—´ä»¶é“¾ï¼Œæœ€åè°ƒç”¨ç”¨æˆ· handler"""
```

**æ¯ä¸ªä¸­é—´ä»¶æ˜¯ä¸€ä¸ª AsyncGenerator**ï¼Œ`yield` å‰æ˜¯åˆå§‹åŒ–ï¼Œ`yield` åæ˜¯æ¸…ç†ã€‚ä¸ Python çš„ `contextmanager` æ¨¡å¼ä¸€è‡´ã€‚å¼‚å¸¸é€šè¿‡ `asend()` ä¼ å›ä¸­é—´ä»¶çš„æ¸…ç†é˜¶æ®µã€‚

### 2.5 Router â€” è¯·æ±‚è·¯ç”± (`router.py`)

```python
class Router(Generic[TCrawlingContext]):
    _default_handler: RequestHandler | None
    _handlers_by_label: dict[str, RequestHandler]

    # åŸºäº request.label åˆ†å‘åˆ°å¯¹åº” handler
    async def __call__(self, context):
        if context.request.label in self._handlers_by_label:
            return await self._handlers_by_label[label](context)
        return await self._default_handler(context)
```

**è®¾è®¡äº®ç‚¹**: Router æœ¬èº«å®ç°äº† `RequestHandler` çš„è°ƒç”¨ç­¾åï¼ˆ`__call__`ï¼‰ï¼Œæ‰€ä»¥å®ƒå¯ä»¥ç›´æ¥ä¼ ç»™ `BasicCrawler(request_handler=router)`ã€‚

### 2.6 Request æ¨¡å‹ (`_request.py`)

```python
class Request(BaseModel):
    url: str
    unique_key: str           # å»é‡é”®
    method: HttpMethod        # GET/POST/...
    headers: HttpHeaders
    payload: HttpPayload | None
    user_data: UserData       # åŒ…å« label, crawlee_data ç­‰
    retry_count: int
    no_retry: bool
    loaded_url: str | None    # é‡å®šå‘åçš„å®é™… URL
    # + CrawleeRequestData: state, session_rotation_count, crawl_depth, session_id, etc.
```

**å…³é”®**: `UserData` æ˜¯ä¸€ä¸ª Pydantic model + MutableMapping æ··åˆä½“ï¼Œ`label` ç”¨äºè·¯ç”±ï¼Œ`__crawlee` å‘½åç©ºé—´å­˜æ”¾æ¡†æ¶å†…éƒ¨å…ƒæ•°æ®ã€‚

### 2.7 Configuration (`configuration.py`)

```python
class Configuration(BaseSettings):  # pydantic-settings
    model_config = SettingsConfigDict(populate_by_name=True)

    internal_timeout: timedelta | None
    log_level: LogLevel = 'INFO'
    purge_on_start: bool = True
    persist_state_interval: timedelta = timedelta(minutes=1)
    max_used_cpu_ratio: float
    max_used_memory_ratio: float
    storage_dir: str = './storage'
    # ... æ‰€æœ‰å­—æ®µæ”¯æŒ CRAWLEE_ å‰ç¼€çš„ç¯å¢ƒå˜é‡
```

**è®¾è®¡**: åŸºäº pydantic-settingsï¼Œæ”¯æŒç¯å¢ƒå˜é‡è¦†ç›– + ç±»å‹éªŒè¯ + é»˜è®¤å€¼ã€‚å…¨å±€å”¯ä¸€å®ä¾‹é€šè¿‡ ServiceLocator ç®¡ç†ã€‚

---

## 3. è¯·æ±‚ç”Ÿå‘½å‘¨æœŸæµç¨‹å›¾

```
ç”¨æˆ·: crawler.run(['https://example.com'])
  â”‚
  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. add_requests()                            â”‚
â”‚    - robots.txt æ£€æŸ¥ (å¦‚æœå¯ç”¨)               â”‚
â”‚    - Request.from_url() åˆ›å»º Request å¯¹è±¡     â”‚
â”‚    - åŠ å…¥ RequestQueue (é€šè¿‡ RequestManager)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. AutoscaledPool.run()                      â”‚
â”‚    å¾ªç¯è°ƒç”¨:                                  â”‚
â”‚    - __is_finished_function() â†’ æ£€æŸ¥é˜Ÿåˆ—ç©º?   â”‚
â”‚    - __is_task_ready_function() â†’ æœ‰å¾…å¤„ç†?   â”‚
â”‚    - __run_task_function() â†’ æ‰§è¡Œä¸‹é¢çš„æµç¨‹    â”‚
â”‚    å¹¶å‘åº¦æ ¹æ®ç³»ç»Ÿèµ„æº (CPU/å†…å­˜) è‡ªåŠ¨è°ƒæ•´       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. __run_task_function()                     â”‚
â”‚    a. request_manager.fetch_next_request()   â”‚
â”‚    b. è·å– Session (å¦‚æœå¯ç”¨ session pool)    â”‚
â”‚    c. è·å– ProxyInfo (å¦‚æœæœ‰ä»£ç†é…ç½®)         â”‚
â”‚    d. åˆ›å»º BasicCrawlingContext              â”‚
â”‚    e. è®°å½•ç»Ÿè®¡å¼€å§‹                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. _run_request_handler(context)             â”‚
â”‚    æ‰§è¡Œ context_pipeline:                    â”‚
â”‚                                              â”‚
â”‚    [BasicCrawler å±‚]                         â”‚
â”‚    â””â”€ _check_url_after_redirects             â”‚
â”‚                                              â”‚
â”‚    [AbstractHttpCrawler å±‚] (HTTP çˆ¬è™«)       â”‚
â”‚    â”œâ”€ _execute_pre_navigation_hooks          â”‚
â”‚    â”œâ”€ _make_http_request                     â”‚
â”‚    â”œâ”€ _handle_status_code_response           â”‚
â”‚    â”œâ”€ _parse_http_response                   â”‚
â”‚    â””â”€ _handle_blocked_request_by_content     â”‚
â”‚                                              â”‚
â”‚    [PlaywrightCrawler å±‚] (æµè§ˆå™¨çˆ¬è™«)        â”‚
â”‚    â”œâ”€ _open_page                             â”‚
â”‚    â”œâ”€ _navigate                              â”‚
â”‚    â”œâ”€ _handle_status_code_response           â”‚
â”‚    â””â”€ _handle_blocked_request_by_content     â”‚
â”‚                                              â”‚
â”‚    æœ€åè°ƒç”¨: router(final_context)            â”‚
â”‚    â†’ ç”¨æˆ·å®šä¹‰çš„ request_handler               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                  â”‚
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                  â”‚
    âœ… æˆåŠŸ              âŒ å¼‚å¸¸
         â”‚                  â”‚
         â–¼                  â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5a. æäº¤ç»“æœ    â”‚  â”‚ 5b. é”™è¯¯å¤„ç†              â”‚
â”‚ - push_data    â”‚  â”‚ - RequestHandlerError     â”‚
â”‚ - add_requests â”‚  â”‚   â†’ _handle_request_error â”‚
â”‚ - KVS å†™å…¥    â”‚  â”‚   â†’ retry_count++         â”‚
â”‚ - mark_handled â”‚  â”‚   â†’ reclaim_request       â”‚
â”‚ - session.     â”‚  â”‚ - SessionError            â”‚
â”‚   mark_good()  â”‚  â”‚   â†’ session.retire()      â”‚
â”‚ - è®°å½•ç»Ÿè®¡å®Œæˆ  â”‚  â”‚   â†’ session_rotation++    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚   â†’ reclaim_request       â”‚
                    â”‚ - è¶…è¿‡ max_retries?        â”‚
                    â”‚   â†’ failed_request_handler â”‚
                    â”‚   â†’ mark_handled + ERROR   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**ä»£ç ä¾æ®**: `__run_task_function()` æ–¹æ³• (line 1385-1508) åŒ…å«å®Œæ•´çš„é”™è¯¯åˆ†æ”¯å¤„ç†ã€‚

---

## 4. æ‰©å±•ç‚¹è®¾è®¡åˆ†æ

### 4.1 æ·»åŠ æ–° Crawler ç±»å‹

**æ–¹å¼**: ç»§æ‰¿ `BasicCrawler` æˆ– `AbstractHttpCrawler`ï¼Œæ³¨å…¥è‡ªå®šä¹‰ `ContextPipeline`ã€‚

```python
# PlaywrightCrawler çš„åšæ³•:
kwargs['_context_pipeline'] = (
    ContextPipeline()
    .compose(self._open_page)
    .compose(self._navigate)
    .compose(self._handle_status_code_response)
    .compose(self._handle_blocked_request_by_content)
)
kwargs['_additional_context_managers'] = [self._browser_pool]
super().__init__(**kwargs)
```

**å…³é”®**: å­ç±»é€šè¿‡ `_context_pipeline` å‚æ•°å®šä¹‰è‡ªå·±çš„ä¸­é—´ä»¶é“¾ï¼Œé€šè¿‡ `_additional_context_managers` æ³¨å…¥éœ€è¦ç”Ÿå‘½å‘¨æœŸç®¡ç†çš„èµ„æºï¼ˆå¦‚æµè§ˆå™¨æ± ï¼‰ã€‚

ç»§æ‰¿å±‚çº§:
```
BasicCrawler
â”œâ”€â”€ AbstractHttpCrawler (+ HttpParser æŠ½è±¡)
â”‚   â”œâ”€â”€ HttpCrawler (åŸå§‹ HTTP)
â”‚   â”œâ”€â”€ BeautifulSoupCrawler (+ BS4 Parser)
â”‚   â”œâ”€â”€ ParselCrawler (+ Parsel Parser)
â”‚   â””â”€â”€ è‡ªå®šä¹‰ HTTP Crawler
â””â”€â”€ PlaywrightCrawler (+ BrowserPool)
    â””â”€â”€ è‡ªå®šä¹‰æµè§ˆå™¨ Crawler
```

### 4.2 æ·»åŠ æ–°å­˜å‚¨åç«¯

**æ–¹å¼**: å®ç° `StorageClient` æŠ½è±¡åŸºç±»ã€‚

```python
class StorageClient(ABC):
    @abstractmethod
    async def create_dataset_client(self, *, id, name, alias, configuration) -> DatasetClient
    @abstractmethod
    async def create_kvs_client(self, *, id, name, alias, configuration) -> KeyValueStoreClient
    @abstractmethod
    async def create_rq_client(self, *, id, name, alias, configuration) -> RequestQueueClient
```

Crawlee å·²å®ç°çš„å­˜å‚¨åç«¯ï¼ˆ`src/crawlee/storage_clients/`ï¼‰:
- `_file_system/` â€” æ–‡ä»¶ç³»ç»Ÿï¼ˆé»˜è®¤ï¼‰
- `_memory/` â€” å†…å­˜
- `_sql/` â€” SQL æ•°æ®åº“
- `_redis/` â€” Redis

æ¯ä¸ªåç«¯éœ€è¦å®ç°ä¸‰ä¸ªå­å®¢æˆ·ç«¯:
- `DatasetClient` â€” push_data / get_data / iterate_items
- `KeyValueStoreClient` â€” get_value / set_value
- `RequestQueueClient` â€” add_batch / fetch_next / mark_handled / reclaim

**æ³¨å†Œ**: é€šè¿‡ `ServiceLocator.set_storage_client()` æˆ–æ„é€  BasicCrawler æ—¶ä¼ å…¥ `storage_client` å‚æ•°ã€‚

### 4.3 æ·»åŠ æ–° HTTP å®¢æˆ·ç«¯

å®ç° `BaseHttpClient` æ¥å£:
```python
class BaseHttpClient(ABC):
    @abstractmethod
    async def send_request(self, url, method, headers, payload, session, proxy_info) -> HttpResponse
```

å·²æœ‰å®ç°: `ImpitHttpClient`ï¼ˆé»˜è®¤ï¼‰ã€`CurlImpersonateHttpClient`ã€`PlaywrightHttpClient`

### 4.4 è‡ªå®šä¹‰ Session åˆ›å»º

```python
SessionPool(
    create_session_function=lambda: Session(
        max_age=timedelta(minutes=30),
        max_error_score=5.0,
    )
)
```

### 4.5 è‡ªå®šä¹‰ä»£ç†ç­–ç•¥

```python
ProxyConfiguration(
    # æ–¹å¼1: URL åˆ—è¡¨è½®è¯¢
    proxy_urls=['http://proxy1:8080', 'http://proxy2:8080'],
    # æ–¹å¼2: åˆ†å±‚ä»£ç†ï¼ˆè‡ªåŠ¨å‡çº§ï¼‰
    tiered_proxy_urls=[['http://cheap:8080'], ['http://premium:8080']],
    # æ–¹å¼3: å®Œå…¨è‡ªå®šä¹‰
    new_url_function=lambda session_id, request: 'http://custom:8080',
)
```

---

## 5. Session å’Œä»£ç†ç®¡ç†

### 5.1 Session (`sessions/_session.py`)

```python
class Session:
    _id: str                    # å”¯ä¸€æ ‡è¯†
    _max_age: timedelta         # æœ€å¤§å­˜æ´»æ—¶é—´ (é»˜è®¤ 50min)
    _max_error_score: float     # æœ€å¤§é”™è¯¯åˆ† (é»˜è®¤ 3.0)
    _error_score_decrement: float  # æˆåŠŸåæ‰£å‡ (é»˜è®¤ 0.5)
    _max_usage_count: int       # æœ€å¤§ä½¿ç”¨æ¬¡æ•° (é»˜è®¤ 50)
    _cookies: SessionCookies    # Cookie ç®¡ç†
    _blocked_status_codes: set  # å°ç¦çŠ¶æ€ç  {401, 403, 429}

    @property
    def is_usable(self) -> bool:
        return not (self.is_blocked or self.is_expired or self.is_max_usage_count_reached)

    def mark_good(self):  # æˆåŠŸ â†’ usage_count++, error_score -= decrement
    def mark_bad(self):   # å¤±è´¥ â†’ error_score += 1, usage_count++
    def retire(self):     # ä¸»åŠ¨é€€å½¹ â†’ error_score += max_error_score
```

### 5.2 SessionPool (`sessions/_session_pool.py`)

```python
class SessionPool:
    _max_pool_size: int = 1000
    # å®ç°ä¸º async context manager
    # æ”¯æŒæŒä¹…åŒ– (é€šè¿‡ RecoverableState + KVS)
    async def get_session(self) -> Session  # éšæœºå–ä¸€ä¸ªå¯ç”¨çš„
    async def get_session_by_id(self, session_id) -> Session | None
```

### 5.3 Session ä¸ Proxy çš„è”åŠ¨

åœ¨ `__run_task_function()` ä¸­:
```python
session = await self._get_session()         # ä»æ± ä¸­å–
proxy_info = await self._get_proxy_info(request, session)  # session.id ä½œä¸º proxy session_id
# â†’ åŒä¸€ä¸ª session æ€»æ˜¯ç»‘å®šåŒä¸€ä¸ª proxy URL
```

---

## 6. é”™è¯¯å¤„ç†å’Œé‡è¯•

### 6.1 é”™è¯¯ç±»å‹å±‚çº§

```python
SessionError                 # è§¦å‘ session è½®è½¬ï¼Œä¸è®¡å…¥ max_request_retries
â”œâ”€â”€ ProxyError               # ä»£ç†é”™è¯¯
RequestHandlerError          # ç”¨æˆ· handler å¼‚å¸¸ï¼ŒåŒ…å« crawling_context
ContextPipelineInitializationError  # ä¸­é—´ä»¶åˆå§‹åŒ–å¤±è´¥
ContextPipelineFinalizationError    # ä¸­é—´ä»¶æ¸…ç†å¤±è´¥
ContextPipelineInterruptedError     # ä¸­é—´ä»¶ä¸»åŠ¨ä¸­æ–­ï¼ˆè·³è¿‡è¯·æ±‚ï¼‰
HttpStatusCodeError          # HTTP çŠ¶æ€ç é”™è¯¯
â”œâ”€â”€ HttpClientStatusCodeError # 4xx å®¢æˆ·ç«¯é”™è¯¯
UserDefinedErrorHandlerError # ç”¨æˆ· error handler è‡ªå·±æŠ›çš„å¼‚å¸¸
RequestCollisionError        # Session å†²çªï¼ˆè¯·æ±‚ç»‘å®šçš„ session å·²å¤±æ•ˆï¼‰
```

### 6.2 é‡è¯•ç­–ç•¥

```python
# ä¸¤å¥—ç‹¬ç«‹çš„é‡è¯•æœºåˆ¶:

# 1. è¯·æ±‚é‡è¯• (max_request_retries, é»˜è®¤ 3)
#    é€‚ç”¨: RequestHandlerError, ContextPipelineInitializationError
#    æµç¨‹: retry_count++ â†’ error_handler(å¯é€‰) â†’ reclaim_request
#    è¶…é™: â†’ failed_request_handler â†’ mark_handled(ERROR)

# 2. Session è½®è½¬ (max_session_rotations, é»˜è®¤ 10)
#    é€‚ç”¨: SessionError, ProxyError
#    æµç¨‹: session.retire() â†’ session_rotation_count++ â†’ reclaim_request
#    ç‹¬ç«‹äºè¯·æ±‚é‡è¯•è®¡æ•°!
```

### 6.3 è‡ªå®šä¹‰é”™è¯¯å¤„ç†

```python
@crawler.error_handler
async def on_error(context, error):
    # åœ¨é‡è¯•ä¹‹å‰è°ƒç”¨
    # å¯ä»¥è¿”å›æ–°çš„ Request æ›¿æ¢å½“å‰è¯·æ±‚

@crawler.failed_request_handler
async def on_failed(context, error):
    # åœ¨æ‰€æœ‰é‡è¯•è€—å°½åè°ƒç”¨
    # å¯ä»¥åšå…œåº•å¤„ç†ï¼Œå¦‚è®°å½•åˆ°æ•°æ®åº“
```

---

## 7. é…ç½®ç®¡ç†

### 7.1 å…¨å±€é…ç½® (Configuration)

åŸºäº `pydantic-settings`ï¼Œæ”¯æŒ:
- ä»£ç ä¸­ç›´æ¥ä¼ å‚: `Configuration(log_level='DEBUG')`
- ç¯å¢ƒå˜é‡: `CRAWLEE_LOG_LEVEL=DEBUG`
- è¿˜æ”¯æŒ `APIFY_` å‰ç¼€ï¼ˆApify å¹³å°å…¼å®¹ï¼‰

### 7.2 ServiceLocator æ¨¡å¼

```python
# å…¨å±€å•ä¾‹
service_locator = ServiceLocator()

# æƒ°æ€§åˆå§‹åŒ– â€” ç¬¬ä¸€æ¬¡ get æ—¶æ‰åˆ›å»ºé»˜è®¤å®ä¾‹
service_locator.get_configuration()   # â†’ Configuration()
service_locator.get_event_manager()   # â†’ LocalEventManager()
service_locator.get_storage_client()  # â†’ FileSystemStorageClient()

# è®¾ç½®è‡ªå®šä¹‰å®ä¾‹ (å¿…é¡»åœ¨ç¬¬ä¸€æ¬¡ get ä¹‹å‰)
service_locator.set_storage_client(RedisStorageClient())
```

**æ³¨æ„**: ä¸€æ—¦æœåŠ¡è¢«è·å–è¿‡ï¼Œå°±ä¸èƒ½å†è®¾ç½®æ–°çš„ï¼ˆæŠ› `ServiceConflictError`ï¼‰ã€‚è¿™ä¿è¯äº†å…¨å±€ä¸€è‡´æ€§ã€‚

### 7.3 Crawler çº§åˆ«é…ç½®è¦†ç›–

BasicCrawler æ„é€ å‡½æ•°å…è®¸è¦†ç›–:
```python
BasicCrawler(
    configuration=Configuration(...),   # è¦†ç›–å…¨å±€é…ç½®
    event_manager=CustomEventManager(), # è¦†ç›–äº‹ä»¶ç®¡ç†å™¨
    storage_client=CustomStorageClient(), # è¦†ç›–å­˜å‚¨åç«¯
    # + æ‰€æœ‰è¿è¡Œæ—¶å‚æ•°å¦‚ max_request_retries, concurrency_settings ç­‰
)
```

æ¯ä¸ª Crawler å®ä¾‹æœ‰è‡ªå·±çš„ `_service_locator` å‰¯æœ¬ã€‚

---

## 8. MCP ç›¸å…³

### 8.1 Crawlee æœ¬èº«æ²¡æœ‰ MCP æ”¯æŒ

Crawlee æœ¬èº«æ˜¯çˆ¬è™«æ¡†æ¶ï¼Œä¸æä¾› MCP server åŠŸèƒ½ã€‚

### 8.2 Apify æœ‰å®˜æ–¹ MCP Server

- **ä»“åº“**: [apify/apify-mcp-server](https://github.com/apify/apify-mcp-server)
- **æœåŠ¡**: mcp.apify.com â€” è®© AI Agent é€šè¿‡ MCP åè®®è°ƒç”¨ Apify å¹³å°ä¸Šçš„ Actorï¼ˆåŒ…æ‹¬ Crawlee æ„å»ºçš„çˆ¬è™«ï¼‰
- **åŸç†**: MCP Server æš´éœ² Apify Actor ä½œä¸º toolsï¼Œä¸æ˜¯ç›´æ¥æ“ä½œ Crawlee å®ä¾‹

### 8.3 ç¤¾åŒºæ–¹æ¡ˆ

æœç´¢ç»“æœæ˜¾ç¤ºç¤¾åŒºä¸»è¦é€šè¿‡ Apify å¹³å° MCP Server é—´æ¥ä½¿ç”¨ Crawlee èƒ½åŠ›ï¼Œæ²¡æœ‰å‘ç°ç‹¬ç«‹çš„ "Crawlee MCP Server"ã€‚

**å¯¹ juanjuan-spider çš„å¯ç¤º**: å¦‚æœéœ€è¦ MCP é›†æˆï¼Œåº”è¯¥æ˜¯åœ¨çˆ¬è™«ä¹‹ä¸Šå°è£…ä¸€å±‚ MCP Serverï¼ˆæš´éœ² `start_crawl`, `get_results`, `list_tasks` ç­‰ toolï¼‰ï¼Œè€Œä¸æ˜¯åœ¨çˆ¬è™«å†…éƒ¨å®ç°ã€‚

---

## 9. å¯¹ juanjuan-spider çš„å…·ä½“å»ºè®®

### 9.1 âœ… å€¼å¾—å­¦çš„

| Crawlee è®¾è®¡ | å»ºè®® | åŸå›  |
|---|---|---|
| **ContextPipeline (ä¸­é—´ä»¶ç®¡é“)** | æ ¸å¿ƒå­¦ä¹  | AsyncGenerator ä¸­é—´ä»¶æ¨¡å¼ä¼˜é›…ï¼Œæ”¯æŒåˆå§‹åŒ–+æ¸…ç†ï¼Œå¼‚å¸¸ä¼ æ’­æ­£ç¡®ã€‚æˆ‘ä»¬å¯ä»¥ç®€åŒ–ä¸º 3-4 å±‚: åçˆ¬æ£€æµ‹ â†’ HTTPè¯·æ±‚ â†’ è§£æ â†’ æ•°æ®æå– |
| **Router (æ ‡ç­¾è·¯ç”±)** | ç›´æ¥é‡‡ç”¨ | ä»£ç æç®€ï¼ˆ~80è¡Œï¼‰ï¼ŒåŸºäº `request.label` åˆ†å‘ã€‚å¯¹æˆ‘ä»¬çš„å¤šå¹³å°çˆ¬è™«ï¼ˆTwitter/Reddit/Polymarketï¼‰éå¸¸é€‚åˆ |
| **Request æ¨¡å‹** | å‚è€ƒç®€åŒ– | Pydantic model ç®¡ç† URL + å…ƒæ•°æ® + å»é‡é”® + çŠ¶æ€æœºã€‚æˆ‘ä»¬å¯ä»¥ç²¾ç®€æ‰ `CrawleeRequestData` ä¸­ä¸éœ€è¦çš„å­—æ®µ |
| **Session ç®¡ç†** | å‚è€ƒè®¾è®¡ | error_score æœºåˆ¶ï¼ˆmark_good/mark_badï¼‰+ è‡ªåŠ¨é€€å½¹ + Cookie ç»‘å®šã€‚å¯¹æˆ‘ä»¬çš„åçˆ¬åœºæ™¯æœ‰ç”¨ |
| **åˆ†å±‚ä»£ç† (tiered_proxy_urls)** | å€¼å¾—å‚è€ƒ | è‡ªåŠ¨æ ¹æ®åŸŸåå°ç¦ç‡å‡çº§ä»£ç†å±‚çº§ï¼Œå·§å¦™ã€‚æˆ‘ä»¬çš„åœºæ™¯å¯èƒ½éœ€è¦ |
| **é”™è¯¯åˆ†ç±»** | é‡‡ç”¨æ€è·¯ | `SessionError` ç‹¬ç«‹äºè¯·æ±‚é‡è¯•ã€`ContextPipelineInterruptedError` è·³è¿‡è¯·æ±‚ â€”â€” ä¸åŒé”™è¯¯èµ°ä¸åŒå¤„ç†è·¯å¾„ |
| **pydantic-settings é…ç½®** | é‡‡ç”¨ | ç¯å¢ƒå˜é‡ + ç±»å‹å®‰å…¨ + é»˜è®¤å€¼ï¼Œä¸€è¡Œæå®š |

### 9.2 âŒ å¤ªé‡ä¸éœ€è¦çš„

| Crawlee è®¾è®¡ | è·³è¿‡åŸå›  |
|---|---|
| **AutoscaledPool + Snapshotter** | æ ¹æ® CPU/å†…å­˜è‡ªåŠ¨è°ƒæ•´å¹¶å‘åº¦ã€‚æˆ‘ä»¬æ˜¯è½»é‡çº§çˆ¬è™«ï¼Œå›ºå®šå¹¶å‘æˆ–ç®€å•çš„ semaphore å°±å¤Ÿäº† |
| **5ç§å­˜å‚¨åç«¯ (FS/Mem/SQL/Redis)** | æˆ‘ä»¬å›ºå®šç”¨ SQLite + æ–‡ä»¶ç³»ç»Ÿå°±å¤Ÿäº†ï¼Œä¸éœ€è¦è¿™ä¹ˆå¤šæŠ½è±¡å±‚ |
| **ServiceLocator å…¨å±€å•ä¾‹** | æˆ‘ä»¬çš„çˆ¬è™«å®ä¾‹å°‘ï¼Œç›´æ¥ä¾èµ–æ³¨å…¥æ›´æ¸…æ™°ã€‚ServiceLocator æ˜¯ä¸ºäº† Apify å¹³å°çš„å¤šå®ä¾‹åœºæ™¯è®¾è®¡çš„ |
| **BrowserPool + æŒ‡çº¹ç”Ÿæˆ** | æˆ‘ä»¬æš‚ä¸éœ€è¦æµè§ˆå™¨çˆ¬è™«ï¼Œå³ä½¿éœ€è¦å¯ä»¥ååŠ  |
| **RequestQueue æŒä¹…åŒ– + æ–­ç‚¹ç»­çˆ¬** | æˆ‘ä»¬çš„ä»»åŠ¡é€šå¸¸å‡ åˆ†é’Ÿå®Œæˆï¼Œä¸éœ€è¦å¤æ‚çš„æŒä¹…åŒ–é˜Ÿåˆ—ã€‚å†…å­˜é˜Ÿåˆ— + ç®€å•é‡è¯•å°±å¤Ÿ |
| **Statistics + ErrorTracker** | æˆ‘ä»¬å¯ä»¥ç”¨ç®€å•çš„è®¡æ•°å™¨ + æ—¥å¿—ï¼Œä¸éœ€è¦å®Œæ•´çš„ç»Ÿè®¡ç³»ç»Ÿ |
| **StorageInstanceManager + ç¼“å­˜** | å¤æ‚çš„å­˜å‚¨å®ä¾‹ç”Ÿå‘½å‘¨æœŸç®¡ç†ã€‚æˆ‘ä»¬ç”¨ç®€å•çš„å·¥å‚æ–¹æ³•å°±å¤Ÿ |

### 9.3 ğŸ¯ æˆ‘ä»¬çš„ç‰¹æ®Šéœ€æ±‚

1. **å¤šå¹³å°ç»Ÿä¸€æ¥å£**: Crawlee çš„ Crawler æŠ½è±¡æ˜¯å›´ç»• "HTTP è¯·æ±‚ â†’ HTML è§£æ" è®¾è®¡çš„ã€‚æˆ‘ä»¬éœ€è¦é€‚é… API-first çš„å¹³å°ï¼ˆTwitter API, Reddit API, Polymarket APIï¼‰ï¼Œè¿™äº›ä¸éœ€è¦ HTML è§£æä½†éœ€è¦è®¤è¯ç®¡ç†ã€Rate Limitingã€åˆ†é¡µé€»è¾‘ã€‚

2. **æ•°æ®æ ‡å‡†åŒ–å±‚**: Crawlee ä¸å…³å¿ƒè¾“å‡ºæ•°æ®çš„æ ¼å¼ï¼ˆpush_data æ˜¯ generic dictï¼‰ã€‚æˆ‘ä»¬éœ€è¦åœ¨ Crawler ä¹‹ä¸ŠåŠ ä¸€å±‚æ•°æ®æ ‡å‡†åŒ–ï¼ˆç»Ÿä¸€çš„ä¿¡å·/ä¿¡æ¯æ¨¡å‹ï¼‰ã€‚

3. **MCP é›†æˆå±‚**: ä½œä¸º MCP tools æš´éœ²ç»™ AI Agentï¼Œéœ€è¦ä¸€ä¸ª MCP Server åŒ…è£…å±‚ã€‚Crawlee æ²¡æœ‰è¿™ä¸ªã€‚

4. **è°ƒåº¦å’Œç¼–æ’**: Crawlee æ˜¯å•æ¬¡ `run()` æ¨¡å‹ï¼ˆè·‘å®Œå°±ç»“æŸï¼‰ã€‚æˆ‘ä»¬éœ€è¦å®šæ—¶/äº‹ä»¶è§¦å‘çš„æŒç»­è¿è¡Œæ¨¡å¼ã€‚

5. **è½»é‡çº§**: æˆ‘ä»¬ä¸éœ€è¦ Crawlee é‚£ç§ "ä¼ä¸šçº§é€šç”¨çˆ¬è™«æ¡†æ¶" çš„å¤æ‚åº¦ã€‚ç›®æ ‡æ˜¯ **å‡ åƒè¡Œä»£ç ** è¦†ç›–æˆ‘ä»¬çš„åœºæ™¯ã€‚

### 9.4 ğŸ“ æ¨èçš„ juanjuan-spider æ¶æ„

åŸºäº Crawlee çš„å¯å‘ï¼Œä½†å¤§å¹…ç®€åŒ–:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              MCP Server Layer                   â”‚
â”‚  æš´éœ² tools: crawl_url, search_topic,          â”‚
â”‚  get_latest_signals, list_sources              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Orchestrator Layer                    â”‚
â”‚  ä»»åŠ¡è°ƒåº¦ / å®šæ—¶è§¦å‘ / å»é‡ / ç»“æœèšåˆ          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Spider Layer (å­¦ Crawlee çš„æ¨¡å¼)       â”‚
â”‚                                                 â”‚
â”‚  BaseSpider (ç±»æ¯” BasicCrawlerï¼Œä½†ç®€åŒ–ç‰ˆ)        â”‚
â”‚  â”œâ”€â”€ ä¸­é—´ä»¶ç®¡é“ (ContextPipeline æ€è·¯)           â”‚
â”‚  â”‚   rate_limit â†’ auth â†’ request â†’ parse        â”‚
â”‚  â”œâ”€â”€ Router (æ ‡ç­¾è·¯ç”±ï¼Œç›´æ¥å­¦ Crawlee)            â”‚
â”‚  â”œâ”€â”€ Session ç®¡ç† (error_score æœºåˆ¶)             â”‚
â”‚  â””â”€â”€ é‡è¯•é€»è¾‘ (SessionError ç‹¬ç«‹äº RequestError) â”‚
â”‚                                                 â”‚
â”‚  å…·ä½“ Spider:                                    â”‚
â”‚  â”œâ”€â”€ HttpSpider (httpx + ç®€å•è§£æ)               â”‚
â”‚  â”œâ”€â”€ ApiSpider (REST API ä¸“ç”¨ï¼Œå«åˆ†é¡µ/è®¤è¯)       â”‚
â”‚  â””â”€â”€ BrowserSpider (Playwright, åæœŸéœ€è¦æ—¶åŠ )     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           Data Layer                            â”‚
â”‚  â”œâ”€â”€ Request (Pydantic model, å­¦ Crawlee)       â”‚
â”‚  â”œâ”€â”€ Result (æ ‡å‡†åŒ–æ•°æ®æ¨¡å‹)                     â”‚
â”‚  â”œâ”€â”€ Storage (SQLite + æ–‡ä»¶, ä¸éœ€è¦æŠ½è±¡å¤šåç«¯)    â”‚
â”‚  â””â”€â”€ Config (pydantic-settings, å­¦ Crawlee)     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 9.5 ğŸ”‘ æ ¸å¿ƒå®ç°ä¼˜å…ˆçº§

1. **P0 â€” Request + Router + ä¸­é—´ä»¶ç®¡é“**: è¿™æ˜¯éª¨æ¶ã€‚ç›´æ¥å‚è€ƒ Crawlee çš„ `Request`ã€`Router`ã€`ContextPipeline`ï¼Œå„ç®€åŒ–åˆ° 100 è¡Œä»¥å†…ã€‚
2. **P0 â€” BaseSpider + HttpSpider**: å®ç° `run()` + ç®€å•å¹¶å‘ï¼ˆ`asyncio.Semaphore`ï¼‰+ é‡è¯•ã€‚
3. **P1 â€” Session + ä»£ç†ç®¡ç†**: å‚è€ƒ Crawlee çš„ `Session` é”™è¯¯è¯„åˆ†æœºåˆ¶ã€‚
4. **P1 â€” ApiSpider**: é’ˆå¯¹ REST API çš„åˆ†é¡µã€è®¤è¯ã€Rate Limit å°è£…ã€‚
5. **P2 â€” MCP Server åŒ…è£…å±‚**ã€‚
6. **P2 â€” è°ƒåº¦ + æŒç»­è¿è¡Œæ¨¡å¼**ã€‚

---

## é™„å½•: Crawlee ä»£ç é‡å‚è€ƒ

| æ–‡ä»¶ | è¡Œæ•° | è¯´æ˜ |
|---|---|---|
| `_basic_crawler.py` | ~1679 | æ ¸å¿ƒå¼•æ“ï¼ŒåŠŸèƒ½æœ€å¯†é›† |
| `_context_pipeline.py` | ~120 | ä¸­é—´ä»¶ç®¡é“ï¼Œä»£ç æç²¾ç‚¼ |
| `router.py` | ~80 | è¯·æ±‚è·¯ç”±ï¼Œæç®€ |
| `_request.py` | ~400 | Request æ¨¡å‹ |
| `configuration.py` | ~200 | å…¨å±€é…ç½® |
| `_session.py` | ~200 | Session ç±» |
| `_session_pool.py` | ~220 | Session æ±  |
| `_service_locator.py` | ~100 | æœåŠ¡å®šä½å™¨ |
| `proxy_configuration.py` | ~200 | ä»£ç†é…ç½® |
| `errors.py` | ~120 | é”™è¯¯ç±»å‹ |

**æ€»ç»“**: Crawlee Python çš„æ ¸å¿ƒä»£ç çº¦ 3000 è¡Œï¼ˆä¸å«å­˜å‚¨åç«¯å’Œæµè§ˆå™¨å±‚ï¼‰ã€‚è®¾è®¡ç²¾è‰¯ä½†åé‡é€šç”¨æ€§ã€‚juanjuan-spider åº”è¯¥å–å…¶ç²¾åï¼ˆContextPipelineã€Routerã€Session error_scoreã€é…ç½®æ¨¡å¼ï¼‰ï¼Œå»å…¶å¤æ‚åº¦ï¼ˆAutoscaledPoolã€å¤šå­˜å‚¨åç«¯ã€ServiceLocatorï¼‰ï¼ŒåŠ ä¸Šæˆ‘ä»¬ç‹¬æœ‰çš„éœ€æ±‚ï¼ˆAPI çˆ¬è™«ã€MCP é›†æˆã€æ•°æ®æ ‡å‡†åŒ–ï¼‰ã€‚ç›®æ ‡ä»£ç é‡æ§åˆ¶åœ¨ 2000 è¡Œä»¥å†…ã€‚