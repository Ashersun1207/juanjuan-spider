"""
juanjuan-spider MCP Server üï∑Ô∏è

ÈÄöËøá MCP ÂçèËÆÆÊö¥Èú≤Áà¨Ëô´ËÉΩÂäõÁªô AI Agent„ÄÇ

ÂêØÂä®ÊñπÂºè:
  python3 -m spider.mcp.server          # stdio Ê®°ÂºèÔºàClaude Desktop / OpenClawÔºâ

Tools:
  spider_scrape      ‚Äî ÊäìÂèñÂçï‰∏™ URLÔºåËøîÂõû markdown/html/text
  spider_batch       ‚Äî ÊâπÈáèÊäìÂèñÂ§ö‰∏™ URL
  spider_query       ‚Äî Êü•ËØ¢ÂéÜÂè≤Áà¨ÂèñËÆ∞ÂΩïÔºàÊåâ URL/ÂüüÂêç/ÂÖ≥ÈîÆËØçÔºâ
  spider_screenshot  ‚Äî ÁΩëÈ°µÊà™Âõæ
"""

from __future__ import annotations

import asyncio
import json
import logging
import re
from typing import Any

from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import TextContent, Tool

from spider.infra.config import SpiderConfig
from spider.storage.sqlite import SpiderStorage

logger = logging.getLogger("spider.mcp")

_config = SpiderConfig()
_storage: SpiderStorage | None = None


def _get_storage() -> SpiderStorage:
    global _storage
    if _storage is None:
        _storage = SpiderStorage(_config.db_path, _config.pages_dir)
    return _storage


async def _do_scrape(
    url: str,
    format: str = "markdown",
    selector: str | None = None,
    wait: float = 0,
    scroll: bool = False,
    max_chars: int = 0,
    save: bool = True,
    no_cache: bool = False,
    screenshot: bool = False,
) -> dict[str, Any]:
    """Ê†∏ÂøÉÊäìÂèñÈÄªËæë ‚Äî Ë∞ÉÁî® main.crawl()Ôºå‰∏çÈáçÂ§çÂÆûÁé∞ÁÆ°ÈÅì„ÄÇ"""
    from spider.core.engine import FetchConfig
    from spider.main import crawl

    fc = FetchConfig(
        wait=wait,
        scroll=scroll,
        selector=selector,
    )

    result = await crawl(
        url,
        save=save,
        no_cache=no_cache,
        fetch_config=fc,
        screenshot=screenshot,
    )

    # ÈÄâÊã©ËæìÂá∫Ê†ºÂºè
    if format == "html":
        content = result.html
    elif format == "fit":
        content = result.fit_markdown or result.markdown
    elif format == "text":
        content = re.sub(r'!?\[([^\]]*)\]\([^)]+\)', r'\1', result.markdown)
        content = re.sub(r'[#*_`~]', '', content)
    else:
        content = result.markdown

    if max_chars > 0 and len(content) > max_chars:
        content = content[:max_chars] + f"\n\n... (Êà™Êñ≠‰∫é {max_chars} Â≠óÁ¨¶)"

    out: dict[str, Any] = {
        "url": result.url,
        "title": result.title,
        "content": content,
        "engine": result.engine,
        "status": result.status,
        "char_count": len(content),
        "duration_ms": result.duration_ms,
    }

    # Êà™Âõæ
    if screenshot and result.screenshot:
        import base64
        out["screenshot_base64"] = base64.b64encode(result.screenshot).decode()
        out["screenshot_bytes"] = len(result.screenshot)

    return out


def create_server() -> Server:
    """ÂàõÂª∫ MCP Server ÂÆû‰æã„ÄÇ"""
    server = Server("juanjuan-spider")

    @server.list_tools()
    async def list_tools() -> list[Tool]:
        return [
            Tool(
                name="spider_scrape",
                description=(
                    "ÊäìÂèñÂçï‰∏™ÁΩëÈ°µÔºåËøîÂõû markdown/html/text„ÄÇ"
                    "ÊîØÊåÅ JS Ê∏≤ÊüìÔºàÂä®ÊÄÅÈ°µÈù¢Ôºâ„ÄÅÂèçÊ£ÄÊµã„ÄÅCSS ÈÄâÊã©Âô®„ÄÅËá™Âä®ÊªöÂä®„ÄÇ"
                    "ÁªìÊûúËá™Âä®ÁºìÂ≠òÂà∞Êú¨Âú∞ SQLite„ÄÇ"
                ),
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "ÁõÆÊ†á URL",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "html", "text", "fit"],
                            "default": "markdown",
                            "description": "ËæìÂá∫Ê†ºÂºèÔºàfit=Êô∫ËÉΩÂéªÂô™ markdownÔºâ",
                        },
                        "selector": {
                            "type": "string",
                            "description": "CSS ÈÄâÊã©Âô®ÔºåÂè™ÊäìÂåπÈÖçÂÜÖÂÆπ",
                        },
                        "wait": {
                            "type": "number",
                            "default": 0,
                            "description": "È¢ùÂ§ñÁ≠âÂæÖÁßíÊï∞ÔºàÁ≠â JS Ê∏≤ÊüìÔºâ",
                        },
                        "scroll": {
                            "type": "boolean",
                            "default": False,
                            "description": "Ëá™Âä®ÊªöÂä®Âä†ËΩΩÊáíÂä†ËΩΩÂÜÖÂÆπ",
                        },
                        "max_chars": {
                            "type": "integer",
                            "default": 0,
                            "description": "ËæìÂá∫ÊúÄÂ§ßÂ≠óÁ¨¶Êï∞Ôºà0=‰∏çÈôêÔºâ",
                        },
                        "no_cache": {
                            "type": "boolean",
                            "default": False,
                            "description": "ÂøΩÁï•ÁºìÂ≠òÔºåÂº∫Âà∂ÈáçÊäì",
                        },
                    },
                    "required": ["url"],
                },
            ),
            Tool(
                name="spider_batch",
                description="ÊâπÈáèÊäìÂèñÂ§ö‰∏™ URLÔºåËøîÂõûÊØè‰∏™ URL ÁöÑÁªìÊûúÊëòË¶Å„ÄÇ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "urls": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "URL ÂàóË°®",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "html", "text", "fit"],
                            "default": "markdown",
                        },
                        "max_chars": {
                            "type": "integer",
                            "default": 5000,
                            "description": "ÊØè‰∏™ URL ÁöÑÊúÄÂ§ßÂ≠óÁ¨¶Êï∞",
                        },
                    },
                    "required": ["urls"],
                },
            ),
            Tool(
                name="spider_query",
                description=(
                    "Êü•ËØ¢ÂéÜÂè≤Áà¨ÂèñËÆ∞ÂΩï„ÄÇÊîØÊåÅÊåâ URL„ÄÅÂüüÂêç„ÄÅÂÖ≥ÈîÆËØçÊêúÁ¥¢Ôºå"
                    "ÊàñÂàóÂá∫ÊúÄËøëÁöÑÁà¨ÂèñËÆ∞ÂΩï„ÄÇ"
                ),
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "Á≤æÁ°Æ URL Êü•ËØ¢",
                        },
                        "domain": {
                            "type": "string",
                            "description": "ÊåâÂüüÂêçÊü•ËØ¢ÔºàÂ¶Ç zhihu.comÔºâ",
                        },
                        "keyword": {
                            "type": "string",
                            "description": "ÊåâÊ†áÈ¢òÊàñ URL Ê®°Á≥äÊêúÁ¥¢",
                        },
                        "limit": {
                            "type": "integer",
                            "default": 10,
                            "description": "ËøîÂõûÊù°Êï∞‰∏äÈôê",
                        },
                    },
                },
            ),
            Tool(
                name="spider_screenshot",
                description="ÂØπÁΩëÈ°µÊà™ÂõæÔºåËøîÂõû base64 ÁºñÁ†ÅÁöÑ PNG ÂõæÁâá„ÄÇ",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "ÁõÆÊ†á URL",
                        },
                        "wait": {
                            "type": "number",
                            "default": 1,
                            "description": "Êà™ÂõæÂâçÁ≠âÂæÖÁßíÊï∞",
                        },
                    },
                    "required": ["url"],
                },
            ),
        ]

    @server.call_tool()
    async def call_tool(name: str, arguments: dict) -> list[TextContent]:
        try:
            if name == "spider_scrape":
                result = await _do_scrape(**arguments)
                return [TextContent(
                    type="text",
                    text=json.dumps(result, ensure_ascii=False, indent=2),
                )]

            elif name == "spider_batch":
                urls = arguments.get("urls", [])
                fmt = arguments.get("format", "markdown")
                max_chars = arguments.get("max_chars", 5000)
                results = []
                for url in urls:
                    try:
                        r = await _do_scrape(
                            url=url, format=fmt, max_chars=max_chars
                        )
                        results.append(r)
                    except Exception as e:
                        logger.warning("batch scrape failed for %s: %s", url, e)
                        results.append({
                            "url": url,
                            "status": "failed",
                            "error": str(e),
                        })
                return [TextContent(
                    type="text",
                    text=json.dumps(results, ensure_ascii=False, indent=2),
                )]

            elif name == "spider_query":
                storage = _get_storage()
                if url := arguments.get("url"):
                    rows = storage.get_by_url(url)
                elif domain := arguments.get("domain"):
                    rows = storage.get_by_domain(
                        domain, limit=arguments.get("limit", 10)
                    )
                elif keyword := arguments.get("keyword"):
                    rows = storage.search(
                        keyword, limit=arguments.get("limit", 10)
                    )
                else:
                    rows = storage.recent(limit=arguments.get("limit", 10))

                summary = []
                for r in rows:
                    summary.append({
                        "url": r["url"],
                        "title": r.get("title", ""),
                        "domain": r.get("domain", ""),
                        "engine": r.get("engine", ""),
                        "status": r.get("status", ""),
                        "char_count": r.get("char_count", 0),
                        "crawled_at": r.get("crawled_at", ""),
                    })
                return [TextContent(
                    type="text",
                    text=json.dumps(summary, ensure_ascii=False, indent=2),
                )]

            elif name == "spider_screenshot":
                url = arguments["url"]
                wait = arguments.get("wait", 1)
                result = await _do_scrape(
                    url=url, format="fit", wait=wait,
                    save=False, screenshot=True,
                )
                if result.get("screenshot_base64"):
                    return [TextContent(
                        type="text",
                        text=json.dumps({
                            "url": url,
                            "screenshot_base64": result["screenshot_base64"],
                            "size_bytes": result.get("screenshot_bytes", 0),
                        }),
                    )]
                return [TextContent(
                    type="text",
                    text=json.dumps({"url": url, "error": "Êà™ÂõæÂ§±Ë¥•"}),
                )]

            else:
                return [TextContent(
                    type="text",
                    text=json.dumps({"error": f"Êú™Áü• tool: {name}"}),
                )]

        except Exception as e:
            logger.error("tool %s failed: %s", name, e, exc_info=True)
            return [TextContent(
                type="text",
                text=json.dumps({"error": str(e)}, ensure_ascii=False),
            )]

    return server


async def main():
    """MCP Server ‰∏ªÂÖ•Âè£Ôºàstdio Ê®°ÂºèÔºâ„ÄÇ"""
    server = create_server()
    async with stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream, server.create_initialization_options())


if __name__ == "__main__":
    asyncio.run(main())
