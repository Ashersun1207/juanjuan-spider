"""
juanjuan-spider MCP Server ğŸ•·ï¸

é€šè¿‡ MCP åè®®æš´éœ²çˆ¬è™«èƒ½åŠ›ç»™ AI Agentã€‚

å¯åŠ¨æ–¹å¼:
  python3 -m spider.mcp.server          # stdio æ¨¡å¼ï¼ˆClaude Desktop / OpenClawï¼‰
  python3 -m spider.mcp.server --sse    # SSE æ¨¡å¼ï¼ˆHTTP è¿œç¨‹è°ƒç”¨ï¼‰

Tools:
  spider_scrape      â€” æŠ“å–å•ä¸ª URLï¼Œè¿”å› markdown/html/text
  spider_batch       â€” æ‰¹é‡æŠ“å–å¤šä¸ª URL
  spider_query       â€” æŸ¥è¯¢å†å²çˆ¬å–è®°å½•ï¼ˆæŒ‰ URL/åŸŸå/å…³é”®è¯ï¼‰
  spider_screenshot  â€” ç½‘é¡µæˆªå›¾
"""

from __future__ import annotations

import asyncio
import json
import sys
from typing import Any

from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import TextContent, Tool

from spider.core.engine import FetchConfig
from spider.core.result import CrawlResult
from spider.engines.crawl4ai_engine import Crawl4AIEngine
from spider.engines.http_engine import HttpEngine
from spider.core.router import Router
from spider.infra.config import SpiderConfig
from spider.storage.sqlite import SpiderStorage

# å…¨å±€å®ä¾‹ï¼ˆMCP server ç”Ÿå‘½å‘¨æœŸå†…å¤ç”¨ï¼‰
_config = SpiderConfig()
_crawl4ai: Crawl4AIEngine | None = None
_http: HttpEngine | None = None
_storage: SpiderStorage | None = None


def _get_storage() -> SpiderStorage:
    global _storage
    if _storage is None:
        _storage = SpiderStorage(_config.db_path, _config.pages_dir)
    return _storage


async def _get_engines() -> tuple[Crawl4AIEngine, HttpEngine]:
    global _crawl4ai, _http
    if _crawl4ai is None:
        _crawl4ai = Crawl4AIEngine()
    if _http is None:
        _http = HttpEngine()
    return _crawl4ai, _http


async def _do_scrape(
    url: str,
    format: str = "markdown",
    selector: str | None = None,
    wait: float = 0,
    scroll: bool = False,
    max_chars: int = 0,
    save: bool = True,
    no_cache: bool = False,
) -> dict[str, Any]:
    """æ ¸å¿ƒæŠ“å–é€»è¾‘ï¼Œä¾›å„ tool å¤ç”¨ã€‚"""
    storage = _get_storage()

    # ç¼“å­˜æ£€æŸ¥
    if save and not no_cache:
        cached = storage.get_cached(url)
        if cached and cached.get("file_path"):
            file_path = _config.storage_dir / cached["file_path"]
            if file_path.exists():
                content = file_path.read_text(encoding="utf-8")
                if max_chars > 0 and len(content) > max_chars:
                    content = content[:max_chars] + f"\n\n... (æˆªæ–­äº {max_chars} å­—ç¬¦)"
                return {
                    "url": cached["url"],
                    "title": cached.get("title", ""),
                    "content": content,
                    "engine": cached.get("engine", ""),
                    "status": "cached",
                    "char_count": len(content),
                }

    # æ„å»ºé…ç½®
    fc = FetchConfig(
        proxy=_config.proxy if _config.use_proxy else None,
        timeout=_config.timeout,
        stealth=_config.stealth,
        headless=_config.headless,
        wait=wait,
        scroll=scroll,
        selector=selector,
    )

    # è·¯ç”± + æŠ“å–
    crawl4ai, http = await _get_engines()
    router = Router(default_engine=crawl4ai, http_engine=http)
    engine, adapter = router.route(url)
    fc = adapter.customize_config(fc)

    result = await engine.fetch(url, fc)
    result = adapter.transform(result)

    # å­˜å‚¨
    if save:
        storage.save(result)

    # é€‰æ‹©è¾“å‡ºæ ¼å¼
    if format == "html":
        content = result.html
    elif format == "fit":
        content = result.fit_markdown or result.markdown
    elif format == "text":
        import re
        content = re.sub(r'!?\[([^\]]*)\]\([^)]+\)', r'\1', result.markdown)
        content = re.sub(r'[#*_`~]', '', content)
    else:
        content = result.markdown

    if max_chars > 0 and len(content) > max_chars:
        content = content[:max_chars] + f"\n\n... (æˆªæ–­äº {max_chars} å­—ç¬¦)"

    return {
        "url": result.url,
        "title": result.title,
        "content": content,
        "engine": result.engine,
        "status": result.status,
        "char_count": len(content),
        "duration_ms": result.duration_ms,
    }


def create_server() -> Server:
    """åˆ›å»º MCP Server å®ä¾‹ã€‚"""
    server = Server("juanjuan-spider")

    @server.list_tools()
    async def list_tools() -> list[Tool]:
        return [
            Tool(
                name="spider_scrape",
                description=(
                    "æŠ“å–å•ä¸ªç½‘é¡µï¼Œè¿”å› markdown/html/textã€‚"
                    "æ”¯æŒ JS æ¸²æŸ“ï¼ˆåŠ¨æ€é¡µé¢ï¼‰ã€åæ£€æµ‹ã€CSS é€‰æ‹©å™¨ã€è‡ªåŠ¨æ»šåŠ¨ã€‚"
                    "ç»“æœè‡ªåŠ¨ç¼“å­˜åˆ°æœ¬åœ° SQLiteã€‚"
                ),
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "ç›®æ ‡ URL",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "html", "text", "fit"],
                            "default": "markdown",
                            "description": "è¾“å‡ºæ ¼å¼ï¼ˆfit=æ™ºèƒ½å»å™ª markdownï¼‰",
                        },
                        "selector": {
                            "type": "string",
                            "description": "CSS é€‰æ‹©å™¨ï¼ŒåªæŠ“åŒ¹é…å†…å®¹",
                        },
                        "wait": {
                            "type": "number",
                            "default": 0,
                            "description": "é¢å¤–ç­‰å¾…ç§’æ•°ï¼ˆç­‰ JS æ¸²æŸ“ï¼‰",
                        },
                        "scroll": {
                            "type": "boolean",
                            "default": False,
                            "description": "è‡ªåŠ¨æ»šåŠ¨åŠ è½½æ‡’åŠ è½½å†…å®¹",
                        },
                        "max_chars": {
                            "type": "integer",
                            "default": 0,
                            "description": "è¾“å‡ºæœ€å¤§å­—ç¬¦æ•°ï¼ˆ0=ä¸é™ï¼‰",
                        },
                        "no_cache": {
                            "type": "boolean",
                            "default": False,
                            "description": "å¿½ç•¥ç¼“å­˜ï¼Œå¼ºåˆ¶é‡æŠ“",
                        },
                    },
                    "required": ["url"],
                },
            ),
            Tool(
                name="spider_batch",
                description="æ‰¹é‡æŠ“å–å¤šä¸ª URLï¼Œè¿”å›æ¯ä¸ª URL çš„ç»“æœæ‘˜è¦ã€‚",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "urls": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "URL åˆ—è¡¨",
                        },
                        "format": {
                            "type": "string",
                            "enum": ["markdown", "html", "text", "fit"],
                            "default": "markdown",
                        },
                        "max_chars": {
                            "type": "integer",
                            "default": 5000,
                            "description": "æ¯ä¸ª URL çš„æœ€å¤§å­—ç¬¦æ•°",
                        },
                    },
                    "required": ["urls"],
                },
            ),
            Tool(
                name="spider_query",
                description=(
                    "æŸ¥è¯¢å†å²çˆ¬å–è®°å½•ã€‚æ”¯æŒæŒ‰ URLã€åŸŸåã€å…³é”®è¯æœç´¢ï¼Œ"
                    "æˆ–åˆ—å‡ºæœ€è¿‘çš„çˆ¬å–è®°å½•ã€‚"
                ),
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "ç²¾ç¡® URL æŸ¥è¯¢",
                        },
                        "domain": {
                            "type": "string",
                            "description": "æŒ‰åŸŸåæŸ¥è¯¢ï¼ˆå¦‚ zhihu.comï¼‰",
                        },
                        "keyword": {
                            "type": "string",
                            "description": "æŒ‰æ ‡é¢˜æˆ– URL æ¨¡ç³Šæœç´¢",
                        },
                        "limit": {
                            "type": "integer",
                            "default": 10,
                            "description": "è¿”å›æ¡æ•°ä¸Šé™",
                        },
                    },
                },
            ),
            Tool(
                name="spider_screenshot",
                description="å¯¹ç½‘é¡µæˆªå›¾ï¼Œè¿”å› base64 ç¼–ç çš„ PNG å›¾ç‰‡ã€‚",
                inputSchema={
                    "type": "object",
                    "properties": {
                        "url": {
                            "type": "string",
                            "description": "ç›®æ ‡ URL",
                        },
                        "wait": {
                            "type": "number",
                            "default": 1,
                            "description": "æˆªå›¾å‰ç­‰å¾…ç§’æ•°",
                        },
                    },
                    "required": ["url"],
                },
            ),
        ]

    @server.call_tool()
    async def call_tool(name: str, arguments: dict) -> list[TextContent]:
        try:
            if name == "spider_scrape":
                result = await _do_scrape(**arguments)
                return [TextContent(
                    type="text",
                    text=json.dumps(result, ensure_ascii=False, indent=2),
                )]

            elif name == "spider_batch":
                urls = arguments.get("urls", [])
                fmt = arguments.get("format", "markdown")
                max_chars = arguments.get("max_chars", 5000)
                results = []
                for url in urls:
                    try:
                        r = await _do_scrape(
                            url=url, format=fmt, max_chars=max_chars
                        )
                        results.append(r)
                    except Exception as e:
                        results.append({
                            "url": url,
                            "status": "failed",
                            "error": str(e),
                        })
                return [TextContent(
                    type="text",
                    text=json.dumps(results, ensure_ascii=False, indent=2),
                )]

            elif name == "spider_query":
                storage = _get_storage()
                if url := arguments.get("url"):
                    rows = storage.get_by_url(url)
                elif domain := arguments.get("domain"):
                    rows = storage.get_by_domain(
                        domain, limit=arguments.get("limit", 10)
                    )
                elif keyword := arguments.get("keyword"):
                    rows = storage.search(
                        keyword, limit=arguments.get("limit", 10)
                    )
                else:
                    rows = storage.recent(limit=arguments.get("limit", 10))

                # ç²¾ç®€è¾“å‡ºï¼ˆä¸è¿”å›å®Œæ•´å†…å®¹ï¼‰
                summary = []
                for r in rows:
                    summary.append({
                        "url": r["url"],
                        "title": r.get("title", ""),
                        "domain": r.get("domain", ""),
                        "engine": r.get("engine", ""),
                        "status": r.get("status", ""),
                        "char_count": r.get("char_count", 0),
                        "crawled_at": r.get("crawled_at", ""),
                    })
                return [TextContent(
                    type="text",
                    text=json.dumps(summary, ensure_ascii=False, indent=2),
                )]

            elif name == "spider_screenshot":
                url = arguments["url"]
                wait = arguments.get("wait", 1)
                result = await _do_scrape(
                    url=url, format="markdown", wait=wait, save=False,
                )
                # æˆªå›¾éœ€è¦å•ç‹¬å¤„ç†
                fc = FetchConfig(
                    proxy=_config.proxy if _config.use_proxy else None,
                    timeout=_config.timeout,
                    stealth=_config.stealth,
                    headless=True,
                    wait=wait,
                )
                crawl4ai, _ = await _get_engines()
                r = await crawl4ai.fetch(url, fc)
                if r.screenshot:
                    import base64
                    b64 = base64.b64encode(r.screenshot).decode()
                    return [TextContent(
                        type="text",
                        text=json.dumps({
                            "url": url,
                            "screenshot_base64": b64,
                            "size_bytes": len(r.screenshot),
                        }),
                    )]
                return [TextContent(
                    type="text",
                    text=json.dumps({"url": url, "error": "æˆªå›¾å¤±è´¥"}),
                )]

            else:
                return [TextContent(
                    type="text",
                    text=json.dumps({"error": f"æœªçŸ¥ tool: {name}"}),
                )]

        except Exception as e:
            return [TextContent(
                type="text",
                text=json.dumps({"error": str(e)}, ensure_ascii=False),
            )]

    return server


async def main():
    """MCP Server ä¸»å…¥å£ï¼ˆstdio æ¨¡å¼ï¼‰ã€‚"""
    server = create_server()
    async with stdio_server() as (read_stream, write_stream):
        await server.run(read_stream, write_stream, server.create_initialization_options())


# æ”¯æŒ python3 -m spider.mcp.server å¯åŠ¨
if __name__ == "__main__":
    asyncio.run(main())
